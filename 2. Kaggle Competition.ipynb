{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kaggle Competition\n",
    "\n",
    "Now it's your turn to determine what machine learning model you want to fit to the data! You may use any machine model you like, including ones that we did not cover in class. Remember, your goal is to win the [Kaggle competition](https://inclass.kaggle.com/c/beer2), so try to get your prediction error down, any way you can!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.5/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from random import randint\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "pd.set_option('display.max_rows', 7)\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn.linear_model import LinearRegression, Ridge\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from xgboost import XGBRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scores = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1\n",
    "\n",
    "Fit _at least_ 5 different models to the training data (`/data/beer_train.csv`). Each model must include at least one categorical and one quantitative input variable. At least one model must use linear regression, and at least one model must use $k$-nearest neighbors. Other than that, you are free to fit any machine learning model you like, with any input variables you like, in your pursuit of the model with the best prediction accuracy. (_Hint:_ You might find it worthwhile to create new input variables out of the descriptions of the beers, which are rich in information.)\n",
    "\n",
    "Estimate the test error of each of the models using cross-validation. Determine which of the models you tried is the best."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'abv', 'available', 'description', 'glass', 'ibu', 'isOrganic',\n",
       "       'name', 'originalGravity', 'srm', 'Light', 'Dark', 'Bitter', 'Sweet',\n",
       "       'Hop', 'Pale', 'Sour', 'IBU', 'Refreshing', 'Citrus', 'Rich', 'Malt',\n",
       "       'IPA', 'Ale', 'Dry', 'Black', 'Balanced', 'German', 'Strong', 'Stout',\n",
       "       'India', 'Imperial', 'Wheat', 'Lager', 'Crisp', 'Traditional', 'Finish',\n",
       "       'Golden', 'Belgian', 'America', 'Flavor', 'Yeast', 'Character',\n",
       "       'Caramel', 'Roast', 'Pumpkin', 'Honey', 'Clove', 'Note', 'Big',\n",
       "       'Barley', 'Tropical', 'Intense', 'Herb', 'Complex', 'Perfect',\n",
       "       'Backbone', 'Subtle', 'Abbey', 'Berlin', 'Fruit', 'Berry', 'Berries',\n",
       "       'ABV', 'Craft', 'Brew', 'Floral', 'Filter', 'Amber', 'Red', 'Gravity',\n",
       "       'Very', 'Extremely', 'Rye', 'India Pale Ale', 'We', 'Name IPA',\n",
       "       'Name Double', 'Name Ale', 'Name Imperial', 'Name Stout', 'Name Light',\n",
       "       'Name Wheat', 'Name Blonde', 'Name Pale'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keywords = ['Light','Dark','Bitter', 'Sweet',\n",
    "            'Hop', 'Pale', 'Sour', 'IBU', \n",
    "            'Refreshing', 'Citrus', 'Rich', 'Malt',\n",
    "            'IPA', 'Ale', 'Dry', 'Black','Balanced',\n",
    "            'German', 'Strong', 'Stout','India', \n",
    "            'Imperial', 'Wheat', 'Lager', 'Crisp',\n",
    "            'Traditional', 'Finish', 'Golden','Belgian',\n",
    "            'America', 'Flavor', 'Yeast', 'Character',\n",
    "            'Caramel', 'Roast', 'Pumpkin','Honey','Clove',\n",
    "            'Note', 'Big','Barley', 'Tropical', \n",
    "            'Intense', 'Herb', 'Complex', 'Perfect', \n",
    "            'Backbone', 'Subtle', 'Abbey', 'Berlin',\n",
    "            'Fruit', 'Berry', 'Berries', 'ABV',\n",
    "            'Craft', 'Brew', 'Floral', 'Filter', 'Amber',\n",
    "            'Red', 'Gravity', 'Very', 'Extremely', 'Rye',\n",
    "            'India Pale Ale', 'We'\n",
    "           ]\n",
    "\n",
    "name_keywords = ['IPA', 'Double', 'Ale', 'Imperial',\n",
    "                 'Stout', 'Light', 'Wheat',\n",
    "                 'Blonde', 'Pale'\n",
    "                ]\n",
    "variables = ['originalGravity','srm','abv']\n",
    "\n",
    "# Unknown Data\n",
    "data_test = pd.read_csv(\"/data/beer_test.csv\")\n",
    "data_test['description'] = data_test['description'].str.lower()\n",
    "data_test['isOrganic'] = (data_test['isOrganic'] == 'Y') * 1\n",
    "data_test['srm'] = pd.to_numeric(data_test['srm'], errors='coerce').fillna(45)\n",
    "data_test['originalGravity'] = pd.to_numeric(data_test['originalGravity'], errors='coerce').fillna(1.05)\n",
    "data_test['abv'] = pd.to_numeric(data_test['abv'], errors='coerce').fillna(6.5)\n",
    "for kw in keywords:\n",
    "    data_test[kw] = (data_test['description'].str.count(kw.lower()).fillna(0)) * 1\n",
    "for nkw in name_keywords:\n",
    "    data_test['Name %s' %nkw] = (data_test['name'].str.contains(nkw.lower()).fillna(False)) * 1\n",
    "\n",
    "\n",
    "#----------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "# Training Data\n",
    "data_train = pd.read_csv(\"/data/beer_train.csv\")\n",
    "data_train = data_train[data_train['ibu'] < 150]\n",
    "data_train['description'] = data_train['description'].str.lower()\n",
    "data_train['description'] = data_train['description'].str.replace('-|\\.|!', ' ')\n",
    "data_train['name'] = data_train['name'].str.lower()\n",
    "data_train['isOrganic'] = (data_train['isOrganic'] == 'Y') * 1\n",
    "data_train['srm'] = pd.to_numeric(data_train['srm'], errors='coerce').fillna(55)\n",
    "data_train['originalGravity'] = pd.to_numeric(data_train['originalGravity'], errors='coerce').fillna(1.05)\n",
    "data_train['abv'] = pd.to_numeric(data_train['abv'], errors='coerce').fillna(6.5)\n",
    "for kw in keywords:\n",
    "    data_train[kw] = ((data_train['description'].str.count(kw.lower()).fillna(0)) * 1 ) + ((data_train['name'].str.count(kw.lower()).fillna(0)) * 1 ) \n",
    "for nkw in name_keywords:\n",
    "    data_train['Name %s' %nkw] = (data_train['name'].str.contains(nkw.lower()).fillna(False)) * 1\n",
    "\n",
    "\n",
    "#----------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "\n",
    "# Add dummy variables for the type of glass\n",
    "glass_types = pd.get_dummies(data_train['glass'])\n",
    "glass_types = glass_types.drop('Flute', axis=1)\n",
    "\n",
    "test_glass_types = pd.get_dummies(data_test['glass'])\n",
    "test_glass_types = test_glass_types.drop('Flute', axis=1)\n",
    "\n",
    "#Add dummy variables for availability of beer\n",
    "avails = pd.get_dummies(data_train['available'])\n",
    "avails = avails.drop('Limited availability.', axis=1)\n",
    "\n",
    "test_avails = pd.get_dummies(data_test['available'])\n",
    "test_avails = test_avails.drop('Limited availability.', axis=1)\n",
    "\n",
    "data_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Normalize abv, srm, gravity to get better results\n",
    "from sklearn import preprocessing\n",
    "\n",
    "for var in variables:\n",
    "    data_train[var] = preprocessing.scale(data_train[var])\n",
    "    data_test[var] = preprocessing.scale(data_test[var])\n",
    "\n",
    "for k in keywords:\n",
    "    data_train[k] = preprocessing.scale(data_train[k])\n",
    "    data_test[k] = preprocessing.scale(data_test[k])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Find very bitter beers and very not bitter beers\n",
    "bitters = data_train[data_train['ibu'] >= 40]\n",
    "non = data_train[data_train['ibu'] <= 25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Look at 500 most common words used in description of bitter beers\n",
    "d = bitters['description'].fillna('')\n",
    "bitter_words = {}\n",
    "\n",
    "for ds in d:\n",
    "    if(len(ds) <= 0):\n",
    "        continue\n",
    "    words = ds.split(' ')\n",
    "    \n",
    "    for word in words:\n",
    "        if word in bitter_words:\n",
    "            bitter_words[word] += 1\n",
    "        else:\n",
    "            bitter_words[word] = 1\n",
    "        \n",
    "bitter_words\n",
    "\n",
    "import operator\n",
    "\n",
    "sorted_bitter = sorted(bitter_words.items(), key=operator.itemgetter(1))\n",
    "sorted_bitter[len(sorted_bitter)-500:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Look at 500 most common words used in description of low ibu beers\n",
    "d = non['description'].fillna('')\n",
    "non_words = {}\n",
    "\n",
    "for ds in d:\n",
    "    if(len(ds) <= 0):\n",
    "        continue\n",
    "    words = ds.split(' ')\n",
    "    \n",
    "    for word in words:\n",
    "        if word in non_words:\n",
    "            non_words[word] += 1\n",
    "        else:\n",
    "            non_words[word] = 1\n",
    "        \n",
    "\n",
    "sorted_non = sorted(non_words.items(), key=operator.itemgetter(1))\n",
    "sorted_non[len(sorted_non)-500:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Linear Regression model, using abv to the seventh power,type of glass, and the availability. \n",
    "model = LinearRegression()\n",
    "abv = ['abv']\n",
    "for i in range(2,8):\n",
    "    new_col = \"abv^%s\" % str(i)\n",
    "    data_train[new_col] = data_train['abv'] ** i\n",
    "    abv.append(new_col)\n",
    "\n",
    "abvs = data_train[abv]\n",
    "\n",
    "X = pd.get_dummies(data_train['available']).drop('Beer is not available.', axis=1)\n",
    "test_variables = pd.concat([X, abvs, glass_types], axis=1)\n",
    "print(-cross_val_score(model,test_variables, data_train['ibu'], cv = 10,\n",
    "          scoring = \"neg_mean_squared_error\").mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# K Nearest Neighbors model. 15 neighbours was found to be the optimal number in this case.\n",
    "model = KNeighborsRegressor(n_neighbors=15)\n",
    "\n",
    "test_variables = pd.concat([data_train[['originalGravity','abv', 'srm']+keywords], glass_types, avails], axis=1)\n",
    "print(-cross_val_score(model,test_variables, data_train['ibu'], cv = 10,\n",
    "          scoring = \"neg_mean_squared_error\").mean())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This cell checks every possible combination of keywords and prints the\n",
    "# combination with the lowest error. Since there are 2^47 possible combinations,\n",
    "# this won't finish running. Probably should not run this cell\n",
    "\"\"\"\n",
    "model = RandomForestRegressor()\n",
    "scores = []\n",
    "\n",
    "# Prints error using every keyword\n",
    "print(-cross_val_score(model,data_train[variables+keywords], data_train['ibu'], cv = 10,\n",
    "          scoring = \"neg_mean_squared_error\").mean())\n",
    "\n",
    "\n",
    "#Loop through all possible combinations of keywords. \n",
    "kws = np.array(keywords)\n",
    "for i in range(2**len(keywords)):\n",
    "    mask = list(format(i, '016b'))\n",
    "    mask = np.array([x=='1' for x in mask])\n",
    "    combo = kws[mask].tolist()\n",
    "    scores.append((-cross_val_score(model,data_train[variables+combo], data_train['ibu'], cv = 10,\n",
    "          scoring = \"neg_mean_squared_error\").mean(), combo))\n",
    "print(min(scores))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This cell will use a pseudorandom number as a mask to randomly choose\n",
    "# combinations of keywords. It then prints the one with the lowest error.\n",
    "model = RandomForestRegressor(n_estimators= 40, min_samples_split=13, min_samples_leaf=2, max_features=14)\n",
    "\n",
    "inputs= ['originalGravity', 'srm', 'abv', \n",
    "         'Light','Dark','Bitter', 'Sweet',\n",
    "         'Hop', 'Pale', 'Sour', 'IBU', 'Brew',\n",
    "         'Refreshing','Citrus', 'Rich', 'Complex',\n",
    "         'IPA', 'Ale', 'Dry', 'Black','Balanced',\n",
    "         'German', 'Strong','India', 'Berlin',\n",
    "         'Wheat', 'Lager','Crisp','Traditional',\n",
    "         'Finish', 'Golden','Belgian','America', \n",
    "         'Flavor', 'Yeast', 'Character','Caramel', \n",
    "         'Roast', 'Pumpkin','Clove', 'Big', 'Gravity',\n",
    "         'Barley', 'Tropical', 'Intense','Perfect',\n",
    "         'Backbone', 'Subtle', 'Abbey', 'Herb', 'ABV',\n",
    "         'Name IPA',  'Name Ale', 'Name Imperial', 'Red', \n",
    "         'Name Light', 'Name Wheat', 'Name Blonde', 'Name Pale',\n",
    "        ]\n",
    "    \n",
    "#Prints score using every keyword in the model\n",
    "print(-cross_val_score(model,data_train[inputs], data_train['ibu'], cv = 10,\n",
    "          scoring = \"neg_mean_squared_error\").mean())\n",
    "\n",
    "\n",
    "#Only checks a given amount of random combinations. In this case, 5.\n",
    "kws = np.array(keywords)\n",
    "for j in range(10):\n",
    "    i = randint(2**len(keywords)-5000,2**len(keywords))\n",
    "    mask = list(format(i, '0%db' % len(keywords)))\n",
    "    mask = np.array([x=='1' for x in mask])\n",
    "    combo = kws[mask].tolist()\n",
    "    scores.append((-cross_val_score(model,data_train[variables+combo], data_train['ibu'], cv = 10,\n",
    "          scoring = \"neg_mean_squared_error\").mean(), combo))\n",
    "    if j%5 == 0:\n",
    "        print(min(scores))\n",
    "\n",
    "print(min(scores))\n",
    "print('Finished')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "191.953859994\n"
     ]
    }
   ],
   "source": [
    "#This is the random forest model tuned for words and for the RF Regressor\n",
    "rf_model = RandomForestRegressor(n_estimators= 150, min_samples_split=3, \n",
    "                              min_samples_leaf=2, max_features=14)\n",
    "\n",
    "inputs= ['originalGravity', 'srm', 'abv', \n",
    "         'Light','Dark','Bitter', 'Sweet',\n",
    "         'Hop', 'Pale', 'Sour', 'IBU',\n",
    "         'Refreshing','Citrus', 'Rich',\n",
    "         'IPA', 'Ale', 'Dry', 'Black',\n",
    "         'German', 'Strong','India', 'Berlin',\n",
    "         'Wheat', 'Lager','Crisp','Traditional',\n",
    "         'Golden','Belgian','America', \n",
    "         'Flavor', 'Yeast', 'Red',\n",
    "         'Roast', 'Pumpkin','Clove', \n",
    "         'Barley', 'Tropical', 'Intense',\n",
    "         'Backbone',  'Abbey', \n",
    "         'Name IPA',  'Name Ale', 'Name Imperial',\n",
    "         'Name Light', 'Name Wheat', 'Name Blonde'\n",
    "        ]\n",
    "    \n",
    "\n",
    "    \n",
    "test_variables = data_train[inputs].join(glass_types)\n",
    "    \n",
    "print(-cross_val_score(rf_model,test_variables, data_train['ibu'], cv = 10,\n",
    "          scoring = \"neg_mean_squared_error\").mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "\n",
    "ada_model = AdaBoostRegressor(xgb_model, n_estimators=300)\n",
    "\n",
    "test_variables = data_train[inputs].join(glass_types)\n",
    "print(-cross_val_score(ada_model,test_variables, data_train['ibu'], cv = 10,\n",
    "          scoring = \"neg_mean_squared_error\").mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Loop used to find optimal values for each relevant parameter.\n",
    "for i in range(1,10, 10):\n",
    "    model = RandomForestRegressor(n_estimators= i, min_samples_split=13, min_samples_leaf=2, max_features=14)\n",
    "    print(i)\n",
    "    print(-cross_val_score(model,data_train[variables+keywords], data_train['ibu'], cv = 10,\n",
    "          scoring = \"neg_mean_squared_error\").mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "gbr_model = GradientBoostingRegressor(n_estimators=250, min_samples_split = 2, learning_rate = 0.4)\n",
    "\n",
    "inputs= ['originalGravity', 'srm', 'abv', \n",
    "         'Light','Dark','Bitter', 'Sweet',\n",
    "         'Hop', 'Pale', 'Sour', 'IBU', 'Brew',\n",
    "         'Refreshing','Citrus', 'Rich', 'Complex',\n",
    "         'IPA', 'Ale', 'Dry', 'Black','Balanced',\n",
    "         'German', 'Strong','India', 'Berlin',\n",
    "         'Wheat', 'Lager','Crisp','Traditional',\n",
    "         'Finish', 'Golden','Belgian','America', \n",
    "         'Flavor', 'Yeast', 'Character','Red',\n",
    "         'Roast', 'Pumpkin','Clove', 'Big', 'Gravity',\n",
    "         'Barley', 'Tropical', 'Intense','Perfect',\n",
    "         'Backbone', 'Subtle', 'Abbey', 'Herb', 'ABV',\n",
    "         'Name IPA',  'Name Ale', 'Name Imperial',\n",
    "         'Name Light', 'Name Wheat', 'Name Blonde', 'Name Pale',\n",
    "        ]\n",
    "    \n",
    " \n",
    "print(-cross_val_score(gbr_model,data_train[inputs].join(glass_types).join(avails), data_train['ibu'], cv = 10,\n",
    "          scoring = \"neg_mean_squared_error\").mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "xgb_model = XGBRegressor(min_child_weight = 7, max_depth=6, gamma=0.4,\n",
    "                     subsample = 0.85, colsample_bytree = 0.55, scale_pos_weight=1,\n",
    "                     n_estimators = 150, nthread=4)\n",
    "\n",
    "inputs= ['originalGravity', 'srm', 'abv', \n",
    "         'Light','Dark','Bitter', 'Sweet',\n",
    "         'Hop', 'Pale', 'Sour', 'IBU', 'Brew',\n",
    "         'Refreshing','Citrus', 'Rich', 'Complex',\n",
    "         'IPA', 'Ale', 'Dry', 'Black','Balanced',\n",
    "         'German', 'Strong','India', 'Berlin',\n",
    "         'Wheat', 'Lager','Crisp','Traditional',\n",
    "         'Finish', 'Golden','Belgian','America', \n",
    "         'Flavor', 'Yeast', 'Character','Red',\n",
    "         'Roast', 'Pumpkin','Clove', 'Big', 'Gravity',\n",
    "         'Barley', 'Tropical', 'Intense','Perfect',\n",
    "         'Backbone', 'Subtle', 'Abbey', 'Herb', 'ABV',\n",
    "         'Name IPA',  'Name Ale', 'Name Imperial', \n",
    "         'Name Light', 'Name Wheat', 'Name Blonde', 'Name Pale',\n",
    "        ]\n",
    "    \n",
    "\n",
    "    \n",
    "test_variables = data_train[inputs].join(glass_types)#.join(avails)\n",
    "    \n",
    "print(-cross_val_score(xgb_model,test_variables, data_train['ibu'], cv = 10,\n",
    "          scoring = \"neg_mean_squared_error\").mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear regression is a decent model if you train it with useful data. It is quick, but falls short compared to other\n",
    "models. \n",
    "\n",
    "K nearest neighbors regression is slightly better than linear regression in this project, using similar training data.\n",
    "\n",
    "The adaboost regressor takes a long time to run if it takes another model to ensemble. The results are not much better than when used alone.\n",
    "\n",
    "I found the random forest model and multilayer perceptron model to have the best results by far, especially when the parameters are fine tuned. Random forest is very comparable to mlp, but takes much less time to run. That is why it is my chosen model. \n",
    "\n",
    "The xgbregressor had the lowest cv score andtakes less time to run than random forest. However, it had a worse mse when submitting to kaggle.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grader Comments\n",
    "\n",
    "- \n",
    "- \n",
    "\n",
    "[This question is worth 30 points]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scores.append(None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2\n",
    "\n",
    "Use the model that you determined to be optimal in Question 1, and predict the IBU for the test data. Export your predictions to a CSV file (using `.to_csv()`) in the format expected by Kaggle (see `/data/beer_test_sample_submission.csv`). Then, upload your predictions to [Kaggle](https://inclass.kaggle.com/c/beer2). You'll be able to see how well you did on the Leaderboard. You can upload as often as twice a day until the contest ends on Tuesday, June 6.\n",
    "\n",
    "The top 5 teams will earn up to 5 bonus points. In addition, the team that wins the competition will get another prize!\n",
    "\n",
    "_Hint:_ Be extra careful when encoding the categorical variables. Make sure your encoding for the test data matches the encoding you used for the training data **exactly**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
       "           max_features=17, max_leaf_nodes=None, min_impurity_split=1e-07,\n",
       "           min_samples_leaf=2, min_samples_split=3,\n",
       "           min_weight_fraction_leaf=0.0, n_estimators=500, n_jobs=1,\n",
       "           oob_score=False, random_state=None, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_model = XGBRegressor(min_child_weight = 7, max_depth=6, gamma=0.4,\n",
    "                     subsample = 0.85, colsample_bytree = 0.55, scale_pos_weight=1,\n",
    "                     n_estimators = 150, nthread=4)\n",
    "\n",
    "inputs= ['originalGravity', 'srm', 'abv', \n",
    "         'Light','Dark','Bitter', 'Sweet',\n",
    "         'Hop', 'Pale', 'Sour', 'IBU', 'Brew',\n",
    "         'Refreshing','Citrus', 'Rich', 'Complex',\n",
    "         'IPA', 'Ale', 'Dry', 'Black','Balanced',\n",
    "         'German', 'Strong','India', 'Berlin',\n",
    "         'Wheat', 'Lager','Crisp','Traditional',\n",
    "         'Finish', 'Golden','Belgian','America', \n",
    "         'Flavor', 'Yeast', 'Character','Red',\n",
    "         'Roast', 'Pumpkin','Clove', 'Big', 'Gravity',\n",
    "         'Barley', 'Tropical', 'Intense','Perfect',\n",
    "         'Backbone', 'Subtle', 'Abbey', 'Herb', 'ABV',\n",
    "         'Name IPA',  'Name Ale', 'Name Imperial', \n",
    "         'Name Light', 'Name Wheat', 'Name Blonde', 'Name Pale',\n",
    "        ]\n",
    "    \n",
    "\n",
    "    \n",
    "fit_variables = data_train[inputs].join(glass_types)\n",
    "    \n",
    "model.fit(fit_variables, data_train['ibu'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 37.97158261,  35.36412532,  22.24054342, ...,  34.26337563,\n",
       "        24.86615651,  54.94756519])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_variables = pd.concat([data_test[inputs], test_glass_types], axis=1)\n",
    "predicted_ibus = model.predict(test_variables)\n",
    "predicted_ibus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "with open('predictions.csv', 'w') as f:  \n",
    "    fieldnames = ['id', 'ibu']\n",
    "    writer = csv.DictWriter(f, fieldnames=fieldnames)\n",
    "\n",
    "    writer.writeheader()    \n",
    "    for i in range(6000, 10753):\n",
    "        writer.writerow({'id':i, 'ibu': predicted_ibus[i-6000]})\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grader Comments\n",
    "\n",
    "- \n",
    "- \n",
    "\n",
    "[This question is worth 20 points]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scores.append(None)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
